{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1 text \n2 text \n3 text \n4 text \n5 text \n6 text \n7 text \n8 text \n9 text \n10 text \n11 text \n12 text \n13 text \n14 text \n15 text \n16 text \n17 text \n18 text \n19 text \n20 text \n21 text \n22 text \n23 text \n24 text \n25 text \n26 text \n27 text \n28 text \n29 text \n30 text \n31 text \n32 text \n33 text \n34 text \n35 text \n36 text \n37 text \n38 text \n39 text \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = f'https://www.olx.ua/nedvizhimost/garazhy-parkovki/'\n",
    "response = requests.get(url, headers={'User-Agent': \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "test = soup.find('table', id = \"offers_table\").find_all('tr', class_ = \"wrap\")\n",
    "k = 0\n",
    "for a in test:\n",
    "    k+=1\n",
    "    title = a.find('h3', class_=\"lheight22 margintop5\").find('strong').text\n",
    "    link = a.find('h3', class_=\"lheight22 margintop5\").find('a').get('href')\n",
    "    price = a.find('p', class_=\"price\").find('strong').text\n",
    "    print(k.__str__() + ' text ' )\n",
    "    data = {\n",
    "        'title': title,\n",
    "        'link': link,\n",
    "        'price': price\n",
    "    }  \n",
    "    with open('garaj.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow((data['title'], data['link'], data['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Firms scraped - 1\n",
      "Firms scraped - 2\n",
      "Firms scraped - 3\n",
      "Firms scraped - 4\n",
      "Firms scraped - 5\n",
      "Firms scraped - 6\n",
      "Firms scraped - 7\n",
      "Firms scraped - 8\n",
      "Firms scraped - 9\n",
      "Firms scraped - 10\n",
      "Firms scraped - 11\n",
      "Firms scraped - 12\n",
      "Firms scraped - 13\n",
      "Firms scraped - 14\n",
      "Firms scraped - 15\n",
      "Firms scraped - 16\n",
      "Firms scraped - 17\n",
      "Firms scraped - 18\n",
      "Firms scraped - 19\n",
      "Firms scraped - 20\n",
      "Firms scraped - 21\n",
      "Firms scraped - 22\n",
      "Firms scraped - 23\n",
      "Firms scraped - 24\n",
      "Firms scraped - 25\n",
      "Firms scraped - 26\n",
      "Firms scraped - 27\n",
      "Firms scraped - 28\n",
      "Firms scraped - 29\n",
      "Firms scraped - 30\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = f'https://finance.yahoo.com/trending-tickers'\n",
    "response = requests.get(url, headers={'User-Agent': \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "finance = soup.find('table', class_='yfinlist-table').find_all('tr', class_='BdT')\n",
    "k = 0\n",
    "mass_link = []\n",
    "LINK = 'https://finance.yahoo.com'\n",
    "for yt in finance:\n",
    "    k += 1\n",
    "    href_link = yt.find('td', class_='data-col0').find('a').get('href')\n",
    "    href_link = LINK + href_link\n",
    "    title = yt.find('td', class_='data-col0').find('a').get('title')\n",
    "    mass_link.append(href_link)\n",
    "    data = {\n",
    "        'title': title,\n",
    "        'href_link': href_link\n",
    "    }\n",
    "    with open('finance.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow((data['title'], data['href_link']))\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open('finance_table.csv', 'a', newline='') as fr:\n",
    "    for firm in mass_link:\n",
    "        try:\n",
    "            count += 1\n",
    "            print(\"Firms scraped - \" + count.__str__())\n",
    "            STATIC_LINK = 'https://finance.yahoo.com/quote/'\n",
    "            STATIC_PART_LINK = '/key-statistics?p='\n",
    "            result = firm.split('p=')[1]\n",
    "            url2 = STATIC_LINK + result + STATIC_PART_LINK + result\n",
    "            response2 = requests.get(url2, headers={'User-Agent': \"Mozilla/5.0 (X11; Linux i686) \"\n",
    "                                                                  \"AppleWebKit/537.17 (KHTML, like Gecko) \"\n",
    "                                                                  \"Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "            soup2 = BeautifulSoup(response2.text, 'html.parser')\n",
    "            table_statistic = soup2\\\n",
    "                .find('div', id='Col1-0-KeyStatistics-Proxy')\\\n",
    "                .find('table', class_='W(100%) Bdcl(c) Mt(10px) Mb(10px)')\\\n",
    "                .find('tbody')\\\n",
    "                .find_all('td', class_='Fz(s) Fw(500) Ta(end) Pstart(10px) Miw(60px)')\n",
    "            name_company = result\n",
    "            market_cap_intraday = table_statistic[0].text\n",
    "            enterprise_value = table_statistic[1].text\n",
    "            trailing = table_statistic[2].text\n",
    "            forward_P_E = table_statistic[3].text\n",
    "            PEG_Ratio = table_statistic[4].text\n",
    "            price_sales_ttm = table_statistic[5].text\n",
    "            price_book_mrq = table_statistic[6].text\n",
    "            enterprise_value_revenue = table_statistic[7].text\n",
    "            enterprise_value_ebitda = table_statistic[8].text\n",
    "            data2 = {\n",
    "                'name_company': result,\n",
    "                'market_cap_intraday': market_cap_intraday,\n",
    "                'enterprise_value': enterprise_value,\n",
    "                'trailing': trailing,\n",
    "                'forward_P_E': forward_P_E,\n",
    "                'PEG_Ratio': PEG_Ratio,\n",
    "                'price_sales_ttm': price_sales_ttm,\n",
    "                'price_book_mrq': price_book_mrq,\n",
    "                'enterprise_value_revenue': enterprise_value_revenue,\n",
    "                'enterprise_value_ebitda': enterprise_value_ebitda\n",
    "            }\n",
    "            writer = csv.writer(fr)\n",
    "            writer.writerow((data2['name_company'], data2['market_cap_intraday'],\n",
    "                                 data2['enterprise_value'], data2['trailing'],\n",
    "                                 data2['forward_P_E'], data2['PEG_Ratio'],\n",
    "                                 data2['price_sales_ttm'], data2['price_book_mrq'],\n",
    "                                 data2['enterprise_value_revenue'], data2['enterprise_value_ebitda']))\n",
    "        except:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Car - 1\nJac\nJ2\nCar - 2\nGeely\nGx2\nCar - 3\nChery\nTiggo\nCar - 4\nRenault\nLogan\nCar - 5\nVolkswagen\nPolo\nCar - 6\nVolkswagen\nPolo\nCar - 7\nToyota\nCamry\nCar - 8\nRenault\nLaguna\nCar - 9\nMitsubishi\nOutlander\nCar - 10\nLexus\nEs\nCar - 11\nVolkswagen\nScirocco\nCar - 12\nMercedes-benz\n111cdi\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "try:\n",
    "    page = 1\n",
    "    k = 0\n",
    "    url = f'https://planetavto.ua/ru/Harkivska/page={page};'\n",
    "    response = requests.get(url, headers={\n",
    "                'User-Agent': \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) \"\n",
    "                              \"Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table_cars = soup.find('section', class_='main_products').findAll('article', class_='product_item')\n",
    "    for car in table_cars:\n",
    "        k += 1\n",
    "        print(f'Car - {k}')\n",
    "        brand_model = car.find('a', class_='prod-link').text\n",
    "        brand_model_str = brand_model.__str__()\n",
    "        t = re.search(r'([\\w-]+) (.+)', brand_model_str)\n",
    "        brand = t.group(1).capitalize()\n",
    "        model = t.group(2).capitalize()\n",
    "        print(brand)\n",
    "        print(model)\n",
    "except Exception as e:\n",
    "    logging.error(f'Failed. - {e}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}